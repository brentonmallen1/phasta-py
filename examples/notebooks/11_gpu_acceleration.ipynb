{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Acceleration in PHASTA\n",
    "\n",
    "This notebook demonstrates how to use GPU acceleration features in PHASTA for improved performance. We'll cover:\n",
    "- GPU device setup and configuration\n",
    "- Performance comparison between CPU and GPU\n",
    "- Best practices for GPU usage\n",
    "- Memory management and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from phasta import FlowSolver, FlowConfig, Mesh, GPUConfig\n",
    "import time\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Device Setup\n",
    "\n",
    "First, let's check available GPU devices and configure PHASTA to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create GPU configuration\n",
    "gpu_config = GPUConfig()\n",
    "\n",
    "# List available devices\n",
    "devices = gpu_config.list_devices()\n",
    "print(\"Available GPU devices:\")\n",
    "for i, device in enumerate(devices):\n",
    "    print(f\"Device {i}: {device['name']}\")\n",
    "    print(f\"  Memory: {device['memory']} GB\")\n",
    "    print(f\"  Compute Capability: {device['compute_capability']}\")\n",
    "    print(f\"  Multi-Processor Count: {device['mp_count']}\")\n",
    "\n",
    "# Configure GPU settings\n",
    "gpu_config.device_id = 0  # Use first GPU\n",
    "gpu_config.memory_fraction = 0.8  # Use 80% of available memory\n",
    "gpu_config.enable_tensor_cores = True  # Enable tensor cores if available\n",
    "gpu_config.precision = 'mixed'  # Use mixed precision for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare CPU and GPU performance for a simple flow simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create base configuration\n",
    "config = FlowConfig()\n",
    "config.domain = {\n",
    "    'width': 1.0,\n",
    "    'height': 1.0,\n",
    "    'depth': 1.0,\n",
    "    'mesh_size': 0.01\n",
    "}\n",
    "\n",
    "config.flow = {\n",
    "    'time_step': 0.001,\n",
    "    'max_time': 1.0\n",
    "}\n",
    "\n",
    "# Create mesh\n",
    "mesh = Mesh.generate_structured_3d(\n",
    "    width=config.domain['width'],\n",
    "    height=config.domain['height'],\n",
    "    depth=config.domain['depth'],\n",
    "    nx=100,\n",
    "    ny=100,\n",
    "    nz=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run CPU simulation\n",
    "print(\"Running CPU simulation...\")\n",
    "cpu_start = time.time()\n",
    "cpu_solver = FlowSolver(config, mesh)\n",
    "cpu_results = cpu_solver.solve()\n",
    "cpu_time = time.time() - cpu_start\n",
    "\n",
    "# Run GPU simulation\n",
    "print(\"\\nRunning GPU simulation...\")\n",
    "gpu_start = time.time()\n",
    "gpu_solver = FlowSolver(config, mesh, gpu_config=gpu_config)\n",
    "gpu_results = gpu_solver.solve()\n",
    "gpu_time = time.time() - gpu_start\n",
    "\n",
    "# Print performance comparison\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"CPU Time: {cpu_time:.2f} seconds\")\n",
    "print(f\"GPU Time: {gpu_time:.2f} seconds\")\n",
    "print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management\n",
    "\n",
    "Let's explore how to manage GPU memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Monitor GPU memory usage\n",
    "memory_stats = gpu_solver.get_memory_stats()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(memory_stats['time'], memory_stats['used_memory'], label='Used Memory')\n",
    "plt.plot(memory_stats['time'], memory_stats['total_memory'], label='Total Memory')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Memory (GB)')\n",
    "plt.title('GPU Memory Usage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Scaling\n",
    "\n",
    "Let's examine how performance scales with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test different GPU configurations\n",
    "gpu_counts = [1, 2, 4]\n",
    "times = []\n",
    "\n",
    "for num_gpus in gpu_counts:\n",
    "    gpu_config.device_ids = list(range(num_gpus))\n",
    "    gpu_solver = FlowSolver(config, mesh, gpu_config=gpu_config)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    gpu_solver.solve()\n",
    "    times.append(time.time() - start_time)\n",
    "\n",
    "# Plot scaling results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gpu_counts, times, 'o-', label='Measured')\n",
    "plt.plot(gpu_counts, [times[0]/n for n in gpu_counts], '--', label='Ideal')\n",
    "plt.xlabel('Number of GPUs')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "plt.title('Multi-GPU Scaling')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "Here are some best practices for GPU acceleration in PHASTA:\n",
    "\n",
    "1. Memory Management:\n",
    "   - Use appropriate memory fraction\n",
    "   - Monitor memory usage\n",
    "   - Free unused resources\n",
    "\n",
    "2. Performance Optimization:\n",
    "   - Use mixed precision when possible\n",
    "   - Enable tensor cores for supported operations\n",
    "   - Optimize data transfer between CPU and GPU\n",
    "\n",
    "3. Multi-GPU Usage:\n",
    "   - Balance workload across GPUs\n",
    "   - Minimize inter-GPU communication\n",
    "   - Use appropriate domain decomposition\n",
    "\n",
    "4. Error Handling:\n",
    "   - Check for GPU errors\n",
    "   - Implement fallback to CPU if needed\n",
    "   - Monitor GPU temperature and power usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try different memory fractions and observe performance\n",
    "2. Experiment with different precision settings\n",
    "3. Test performance with different mesh sizes\n",
    "4. Compare CPU and GPU results for accuracy\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try the parallel computing example\n",
    "- Explore advanced visualization techniques\n",
    "- Learn about basic mesh generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 
